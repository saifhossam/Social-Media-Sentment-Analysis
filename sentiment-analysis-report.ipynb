{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T15:16:44.193222Z","iopub.execute_input":"2025-07-14T15:16:44.193833Z","iopub.status.idle":"2025-07-14T15:16:44.199021Z","shell.execute_reply.started":"2025-07-14T15:16:44.193809Z","shell.execute_reply":"2025-07-14T15:16:44.198399Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom datetime import datetime\nimport re\nimport nltk\nfrom nltk.tokenize import word_tokenize\n\n# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù„Ø§Ø²Ù…Ø© Ù„Ù€ NLTK\nnltk.download('punkt')\n\n# Ù‚Ø§Ø¦Ù…Ø© Ù…Ø®ØµØµØ© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\nARABIC_STOPWORDS = {\n    'Ù…Ù†', 'ÙÙŠ', 'Ø¹Ù„Ù‰', 'Ø¥Ù„Ù‰', 'Ø¹Ù†', 'Ù…Ø¹', 'Ø¨ÙŠÙ†', 'Ø­ØªÙ‰', 'Ù…Ù†Ø°', 'Ø¹Ù†Ø¯', 'ÙÙˆÙ‚', 'ØªØ­Øª',\n    'Ø£Ù…Ø§Ù…', 'Ø®Ù„Ù', 'Ù‡Ù†Ø§', 'Ù‡Ù†Ø§Ùƒ', 'ÙƒÙ„', 'Ø¨Ø¹Ø¶', 'Ø£ÙŠ', 'Ù‡Ø°Ø§', 'Ù‡Ø°Ù‡', 'ØªÙ„Ùƒ', 'Ø°Ù„Ùƒ',\n    'Ø§Ù„Ø°ÙŠ', 'Ø§Ù„ØªÙŠ', 'Ø§Ù„Ù„Ø°ÙŠÙ†', 'Ø§Ù„Ù„Ø§ØªÙŠ', 'Ø¥Ù†', 'Ø£Ù†', 'Ù„ÙƒÙ†', 'Ùˆ', 'Ø£Ùˆ', 'Ù„Ø§', 'Ù„Ù…',\n    'Ù„Ù†', 'Ù…Ø§', 'Ù…Ø§Ø°Ø§', 'ÙƒÙŠÙ', 'Ù…ØªÙ‰', 'Ø£ÙŠÙ†', 'Ù„Ù…Ø§Ø°Ø§'\n}\n\n# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø­Ø³Ù†\nmodel_name = \"CAMeL-Lab/bert-base-arabic-camelbert-msa-sentiment\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name)\nsentiment_analyzer = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n\n# Ø¯Ø§Ù„Ø© Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ\ndef clean_text(text):\n    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„ØªØ¹Ø¨ÙŠØ±ÙŠØ© ÙˆØ§Ù„Ø±Ù…ÙˆØ² ØºÙŠØ± Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©\n    text = re.sub(r'[^\\u0621-\\u064A\\s0-9]', '', text)\n    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ø­Ø±ÙˆÙ ØµØºÙŠØ±Ø©\n    text = text.lower()\n    # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in ARABIC_STOPWORDS]\n    return ' '.join(tokens)\n\n# Ø¯Ø§Ù„Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ø¹ Ø¹ØªØ¨Ø© Ø«Ù‚Ø©\ndef analyze_sentiments(comments, confidence_threshold=0.7):\n    results = []\n    for comment in comments:\n        cleaned_comment = clean_text(comment)\n        result = sentiment_analyzer(cleaned_comment)[0]\n        label = result['label']\n        score = result['score']\n        \n        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø¥Ù„Ù‰ Ø¥ÙŠØ¬Ø§Ø¨ÙŠ/Ø³Ù„Ø¨ÙŠ/Ù…Ø­Ø§ÙŠØ¯\n        if label == 'positive' and score >= confidence_threshold:\n            sentiment = 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ'\n        elif label == 'negative' and score >= confidence_threshold:\n            sentiment = 'Ø³Ù„Ø¨ÙŠ'\n        elif label == 'neutral' or score < confidence_threshold:\n            sentiment = 'Ù…Ø­Ø§ÙŠØ¯'\n        else:\n            sentiment = 'Ù…Ø­Ø§ÙŠØ¯'  # ÙÙŠ Ø­Ø§Ù„Ø© Ø¹Ø¯Ù… Ø§Ù„ÙŠÙ‚ÙŠÙ†\n        results.append({'ØªØ¹Ù„ÙŠÙ‚': comment, 'ØªØ¹Ù„ÙŠÙ‚_Ù…Ù†Ø¸Ù': cleaned_comment, 'Ø§Ù„Ù…Ø´Ø§Ø¹Ø±': sentiment, 'Ø§Ù„Ø«Ù‚Ø©': score})\n    return pd.DataFrame(results)\n\n# Ø¯Ø§Ù„Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©\ndef get_common_words(df, sentiment, n=5):\n    words = []\n    for comment in df[df['Ø§Ù„Ù…Ø´Ø§Ø¹Ø±'] == sentiment]['ØªØ¹Ù„ÙŠÙ‚_Ù…Ù†Ø¸Ù']:\n        words.extend(word_tokenize(comment))\n    return Counter(words).most_common(n)\n\n# Ø¯Ø§Ù„Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ø­Ø³Ù†\ndef generate_report(df, output_dir=\"reports\"):\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª\n    sentiment_counts = Counter(df['Ø§Ù„Ù…Ø´Ø§Ø¹Ø±'])\n    total_comments = len(df)\n    positive_pct = (sentiment_counts['Ø¥ÙŠØ¬Ø§Ø¨ÙŠ'] / total_comments * 100) if 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ' in sentiment_counts else 0\n    negative_pct = (sentiment_counts['Ø³Ù„Ø¨ÙŠ'] / total_comments * 100) if 'Ø³Ù„Ø¨ÙŠ' in sentiment_counts else 0\n    neutral_pct = (sentiment_counts['Ù…Ø­Ø§ÙŠØ¯'] / total_comments * 100) if 'Ù…Ø­Ø§ÙŠØ¯' in sentiment_counts else 0\n    \n    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©\n    positive_words = get_common_words(df, 'Ø¥ÙŠØ¬Ø§Ø¨ÙŠ')\n    negative_words = get_common_words(df, 'Ø³Ù„Ø¨ÙŠ')\n    neutral_words = get_common_words(df, 'Ù…Ø­Ø§ÙŠØ¯')\n    \n    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±\n    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n    report_content = f\"\"\"\n# ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø­Ø³Ù†\n## ØªØ§Ø±ÙŠØ® Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {timestamp}\n\n### Ù…Ù„Ø®Øµ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª\n- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª**: {total_comments}\n- **Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©**: {sentiment_counts.get('Ø¥ÙŠØ¬Ø§Ø¨ÙŠ', 0)} ({positive_pct:.2f}%)\n- **Ø³Ù„Ø¨ÙŠØ©**: {sentiment_counts.get('Ø³Ù„Ø¨ÙŠ', 0)} ({negative_pct:.2f}%)\n- **Ù…Ø­Ø§ÙŠØ¯Ø©**: {sentiment_counts.get('Ù…Ø­Ø§ÙŠØ¯', 0)} ({neutral_pct:.2f}%)\n\n### Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©\n| Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ | Ø§Ù„Ù…Ø´Ø§Ø¹Ø± | Ø§Ù„Ø«Ù‚Ø© |\n|----------|----------|--------|\n\"\"\"\n    for _, row in df.iterrows():\n        report_content += f\"| {row['ØªØ¹Ù„ÙŠÙ‚']} | {row['Ø§Ù„Ù…Ø´Ø§Ø¹Ø±']} | {row['Ø§Ù„Ø«Ù‚Ø©']:.2f} |\\n\"\n\n    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±\n    report_path = f\"{output_dir}/sentiment_report_{timestamp}.md\"\n    with open(report_path, 'w', encoding='utf-8') as f:\n        f.write(report_content)\n    \n    # Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø³Ù… Ø¨ÙŠØ§Ù†ÙŠ\n    plt.figure(figsize=(8, 6))\n    sns.barplot(x=list(sentiment_counts.keys()), y=list(sentiment_counts.values()))\n    plt.title(\"ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª\")\n    plt.xlabel(\"Ø§Ù„Ù…Ø´Ø§Ø¹Ø±\")\n    plt.ylabel(\"Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª\")\n    plt_path = f\"{output_dir}/sentiment_plot_{timestamp}.png\"\n    plt.savefig(plt_path)\n    plt.close()\n    \n    return report_path, plt_path\n\n# Ù…Ø«Ø§Ù„ Ù„Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª\nif __name__ == \"__main__\":\n    comments = [\n        \"Ø§Ù„Ù…Ù†ØªØ¬ Ø±Ø§Ø¦Ø¹ ÙˆØ³Ù‡Ù„ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…! ðŸ˜Š\",\n        \"Ø®Ø¯Ù…Ø© Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø³ÙŠØ¦Ø© Ø¬Ø¯Ù‹Ø§ ÙˆØºÙŠØ± Ù…ØªØ¹Ø§ÙˆÙ†Ø© ðŸ˜¡\",\n        \"Ø§Ù„Ù…Ù†ØªØ¬ Ø¹Ø§Ø¯ÙŠØŒ Ù„Ø§ Ø¨Ø£Ø³ Ø¨Ù‡ ðŸ˜\",\n        \"ØªØ¬Ø±Ø¨Ø© Ù…Ù…ØªØ§Ø²Ø©ØŒ Ø³Ø£Ø´ØªØ±ÙŠ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ ðŸ‘\",\n        \"Ø§Ù„ØªÙˆØµÙŠÙ„ ØªØ£Ø®Ø± ÙƒØ«ÙŠØ±Ù‹Ø§ ðŸ˜ž\"\n    ]\n    \n    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±\n    df = analyze_sentiments(comments)\n    \n    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±\n    report_path, plot_path = generate_report(df)\n    print(f\"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {report_path}\")\n    print(f\"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ ÙÙŠ: {plot_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T16:14:49.800521Z","iopub.execute_input":"2025-07-14T16:14:49.801097Z","iopub.status.idle":"2025-07-14T16:14:51.314303Z","shell.execute_reply.started":"2025-07-14T16:14:49.801074Z","shell.execute_reply":"2025-07-14T16:14:51.313497Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\nDevice set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: reports/sentiment_report_2025-07-14_16-14-51.md\nØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø±Ø³Ù… Ø§Ù„Ø¨ÙŠØ§Ù†ÙŠ ÙÙŠ: reports/sentiment_plot_2025-07-14_16-14-51.png\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/seaborn/_oldcore.py:1765: FutureWarning: unique with argument that is not not a Series, Index, ExtensionArray, or np.ndarray is deprecated and will raise in a future version.\n  order = pd.unique(vector)\n","output_type":"stream"}],"execution_count":77},{"cell_type":"code","source":"import shutil\n\nshutil.rmtree('/kaggle/working/reports')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T16:13:36.645280Z","iopub.execute_input":"2025-07-14T16:13:36.646188Z","iopub.status.idle":"2025-07-14T16:13:36.650863Z","shell.execute_reply.started":"2025-07-14T16:13:36.646164Z","shell.execute_reply":"2025-07-14T16:13:36.650181Z"}},"outputs":[],"execution_count":76}]}